## Description

Implementation HAN module of "Document-Level Neural Machine Translation with Hierarchical Attention Networks".

## Source
>	HierarquicalContext.py

## Test files
The output files of the 3 reported systems: transformer NMT, cache NMT, HAN-decoder NMT, HAN-encoder NMT, HAN-encoder-decoder NMT.
>	- sub_es-en: Opensubtitles 
>	- sub_zh-en: TV subtitles 
>	- TED_es-en: TED Talks WIT 2015
>	- TED_zh-en: TED Talks WIT 2014


## Reference:
>Miculicich, L., Ram, D., Pappas, N. & Henderson, J. Document-Level Neural Machine Translation with Hierarchical Attention Networks. EMNLP 2018.

```
@INPROCEEDINGS{Miculicich_EMNLP_2018,
         author = {Miculicich, Lesly and Ram, Dhananjay and Pappas, Nikolaos and Henderson, James},
       projects = {Idiap, SUMMA},
          title = {Document-Level Neural Machine Translation with Hierarchical Attention Networks},
      booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
           year = {2018},
            pdf = {http://publications.idiap.ch/downloads/papers/2018/Miculicich_EMNLP_2018.pdf}
}
```
 
## Contact:
lmiculicich@idiap.ch
